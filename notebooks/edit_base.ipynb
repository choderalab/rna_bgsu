{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66882614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil\n",
    "import pathlib\n",
    "import glob as glob\n",
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "import warnings\n",
    "from pdbfixer import PDBFixer\n",
    "from openmm.app import PDBxFile, PDBFile\n",
    "from openmm.unit import *\n",
    "import mdtraj as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0ee03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(output_path):\n",
    "    \"\"\"\n",
    "    Create output directory\n",
    "    \"\"\"\n",
    "    if os.path.isdir(output_path):\n",
    "        print(\">remove directory\")\n",
    "        shutil.rmtree(output_path)\n",
    "    \n",
    "    pathlib.Path(output_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "593047fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file(files):    \n",
    "    curated_files = []\n",
    "    \n",
    "    for file in files:\n",
    "        n_warnings = 0\n",
    "        basename = os.path.basename(file)\n",
    "\n",
    "\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.simplefilter(\"default\")\n",
    "            fixer = PDBFixer(filename=file)\n",
    "\n",
    "\n",
    "        # check missing atoms\n",
    "        fixer.findMissingResidues()\n",
    "        fixer.findMissingAtoms()\n",
    "        if fixer.missingAtoms:\n",
    "            n_warnings = 1            \n",
    "            print(\"{}: Missing atoms\\n{}\".format(basename, fixer.missingAtoms))\n",
    "            \n",
    "            fixer.addMissingAtoms()\n",
    "            shutil.move(file, file + \".missingAtoms\")\n",
    "            PDBFile.writeFile(fixer.topology, fixer.positions, open(file, \"w\"))\n",
    "\n",
    "        # check duplicate atoms (same residue ID)\n",
    "        x = [ x for x in w if \"duplicate atom\" in str(x) ]\n",
    "        if len(x) != 0:\n",
    "            n_warnings = 2        \n",
    "            print(\"{}: Duplicate atoms -> Renumber residue ID\".format(basename))\n",
    "\n",
    "        if n_warnings == 0:\n",
    "            curated_files.append(file)\n",
    "\n",
    "    return curated_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7effea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_file(output_path, file):    \n",
    "    basename = os.path.basename(file)    \n",
    "    dumpfile = os.path.join(output_path, \"dump\", basename)\n",
    "    outfile = os.path.join(output_path, basename)\n",
    "\n",
    "    # load pdb, check connectivity, and handle 5'\n",
    "    fixer = PDBFixer(filename=file)\n",
    "    \n",
    "    #PDBFile.writeFile(fixer.topology, fixer.positions, open(dumpfile, 'w'), keepIds=True)       # save cif as pdb (dump)\n",
    "    PDBFile.writeFile(fixer.topology, fixer.positions, open(dumpfile, 'w'))       # save cif as pdb (dump)\n",
    "\n",
    "    \n",
    "    atom_indices = [ atom.index for atom in fixer.topology.atoms() if atom.name in [\"O3'\", \"P\"] ]\n",
    "    indice_pairs = list(itertools.combinations(atom_indices, 2))\n",
    "    #print(indice_pairs)\n",
    "\n",
    "    d = []\n",
    "    for i in indice_pairs:\n",
    "        d.append(calc_dist(fixer, i))\n",
    "\n",
    "    if max(d) < 2:\n",
    "        print(\"{} maybe connected\")\n",
    "    else:\n",
    "        indices = []\n",
    "        indices = [ atom.index for atom in fixer.topology.atoms() if atom.name not in [\"P\", \"OP1\", \"OP2\"] and atom.element.symbol != \"H\" ]   # exclude P, OP1, and OP2 to handle as 5' base residue\n",
    "\n",
    "        traj = md.load_pdb(dumpfile, atom_indices=indices)\n",
    "        traj.save_pdb(outfile)\n",
    "        \n",
    "        # insert TER\n",
    "        t = md.load(outfile)\n",
    "        if t.topology.n_chains != t.topology.n_residues:\n",
    "            insert_ter(t, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f65fdff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_ter(t, f):\n",
    "    \n",
    "    # read\n",
    "    c = 0\n",
    "    ref = None\n",
    "    arr = []\n",
    "    with open(f, \"r\") as rf:\n",
    "        for l in rf.readlines():\n",
    "            _l = l.strip('\\n').split()\n",
    "            if l.startswith(\"ATOM\"):\n",
    "                aname = _l[2]\n",
    "                if ref == None:\n",
    "                    ref = _l[2]\n",
    "                    \n",
    "                if aname == ref and c > 0:\n",
    "                    arr.append(\"TER\\n\")\n",
    "                    arr.append(l)\n",
    "                else:\n",
    "                    arr.append(l)\n",
    "                    c += 1\n",
    "            elif l.startswith(\"TER\"):\n",
    "                pass\n",
    "            else:\n",
    "                arr.append(l)\n",
    "                \n",
    "                \n",
    "    # write\n",
    "    with open(f, \"w\") as wf:\n",
    "        for a in arr:\n",
    "            wf.write(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cd44de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dist(fixer, i):\n",
    "    x = fixer.positions[i[0]] - fixer.positions[i[1]]\n",
    "    d = sum([ v**2 for v in x.value_in_unit(angstroms) ])\n",
    "    \n",
    "    return sqrt(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57fa2c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_path = os.path.dirname(os.path.abspath(\"__file__\")).strip('notebooks')\n",
    "    release_versions = [ \"bpcatalog\", \"triplebase\" ]\n",
    "    #release_versions = [ \"triplebase\" ]\n",
    "        \n",
    "    # create directory\n",
    "    for release_version in release_versions:\n",
    "        #print(\">{}\".format(release_version))\n",
    "        output_path = os.path.join(base_path, \"pdb\", release_version)\n",
    "        make_dir(output_path)\n",
    "        make_dir(os.path.join(output_path, \"dump\"))\n",
    "        \n",
    "        files = glob.glob(os.path.join(base_path) + \"data/{}/*/*.pdb\".format(release_version))\n",
    "        #files = [ file for file in files if file.find(\"exemplar\") == -1 ]   # remove exemplar files from triplebase\n",
    "\n",
    "        curated_files = check_file(files)    \n",
    "        \n",
    "        for file in curated_files:\n",
    "            export_file(output_path, file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae84df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0902369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
